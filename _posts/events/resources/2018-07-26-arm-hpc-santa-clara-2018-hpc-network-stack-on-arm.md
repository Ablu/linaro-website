---
date: 2018-07-26 09:00:00+00:00
categories:
- event_resources
tags:
- Arm
- HPC
- Workshop
- Santa Clara
keywords: Arm, HPC, Workshop, Santa Clara
event: arm-hpc-santa-clara-2018
image: /assets/images/content/ArmHPCWorkshopSocialMedia.png
title: HPC network stack on ARM
speakers:
 - name: Pavel Shamis
   company: Arm
   image: ""
   email: ""
   bio: >
    Pavel is a Principal Research Engineer at Arm with over 16 years of experience in development HPC solutions. His work is focused on co-design software and hardware building blocks for high-performance interconnect technologies, development communication middleware and novel programming models. Prior to joining ARM, he spent five years at Oak Ridge National Laboratory (ORNL) as a research scientist at Computer Science and Math Division (CSMD). In this role, Pavel was responsible for research and development multiple projects in high-performance communication domain including: Collective Communication Offload (CORE-Direct & Cheetah), OpenSHMEM, and OpenUCX. Before joining ORNL, Pavel spent ten years at Mellanox Technologies, where he led Mellanox HPC team and was one of the key driver in enablement Mellanox HPC software stack, including OFA software stack, OpenMPI, MVAPICH, OpenSHMEM, and other.

    Pavel is a recipient of prestigious R&D100 award for his contribution in development of the CORE-Direct collective offload technology and he published in excess of 20 research papers.
slideshare: https://www.slideshare.net/linaroorg/hpc-network-stack-on-arm-linaro-hpc-workshop-2018
s3_video: ""
amazon_s3_presentation_url: ""
youtube_video_url: ""
---
Applications, programming languages, and libraries that leverage sophisticated network hardware capabilities have a natural advantage when used in today¹s and tomorrow's high-performance and data center computer environments. Modern RDMA based network interconnects provides incredibly rich functionality (RDMA, Atomics, OS-bypass, etc.) that enable low-latency and high-bandwidth communication services. The functionality is supported by a variety of interconnect technologies such as InfiniBand, RoCE, iWARP, Intel OPA, Cray¹s Aries/Gemini, and others. Over the last decade, the HPC community has developed variety user/kernel level protocols and libraries that enable a variety of high-performance applications over RDMA interconnects including MPI, SHMEM, UPC, etc. With the emerging availability HPC solutions based on Arm CPU architecture it is important to understand how Arm integrates with the RDMA hardware and HPC network software stack. In this talk, we will overview Arm architecture and system software stack, including MPI runtimes, OpenSHMEM, and OpenUCX.
